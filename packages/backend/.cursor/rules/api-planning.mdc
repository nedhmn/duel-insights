---
alwaysApply: false
---

# Duel Insights API - Comprehensive Planning Document

## Overview

This FastAPI application serves as the backend for a NextJS frontend, focusing on submitting, processing, saving, and outputting jobs via Celery workers and AWS S3. The main functionality revolves around scraping and analyzing DuelingBook replay data.

## Tech Stack

- **Framework**: FastAPI
- **Authentication**: JWT auth via Clerk's JWT template
- **Task Queue**: Celery workers
- **Message Broker & Cache**: Redis (Celery broker/backend + result caching)
- **Storage**: AWS S3
- **Database**: PostgreSQL
- **Web Automation**: Playwright (for BrightData scraping)
- **Testing**: pytest
- **Infrastructure**: Railway (Backend + PostgreSQL + Redis)

## Core Features

### Job Types

#### 1. Individual Mode

- Users submit a list of URLs to analyze one player
- All replays must be from the same player
- Scrapes, parses, and transforms data game-by-game
- Outputs series summaries

#### 2. GFWL (Goat Format War League) Mode

- Users submit team results JSON with week-to-week URLs
- System finds all players in the matches
- User confirms which players to profile
- Creates comprehensive player profiles

## Data Models

### Design Philosophy

**Raw Data Storage + On-Demand Transformation Approach:**

- **Raw JSON Storage**: All scraped data stored as raw JSON in S3, referenced by ScrapedData table
- **No Result Duplication**: Job results not stored in database to avoid data duplication across overlapping URLs
- **On-Demand Processing**: Results computed in real-time when requested, allowing for flexible aggregation
- **Efficient Caching**: Transformed results cached in Redis with TTL to balance performance and freshness
- **Flexible Analysis**: Users can request different player combinations for GFWL jobs without re-scraping

### Core Models

```python
class JobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class JobType(str, Enum):
    INDIVIDUAL = "individual"
    GFWL = "gfwl"

class Job(BaseModel):
    __tablename__ = "jobs"

    job_type: Mapped[JobType]
    status: Mapped[JobStatus] = mapped_column(default=JobStatus.PENDING)
    user_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("users.id"))

    # Input data
    urls: Mapped[list[str]] = mapped_column(JSON)
    team_data: Mapped[dict | None] = mapped_column(JSON, nullable=True)  # For GFWL mode

    # Progress tracking
    total_urls: Mapped[int]
    processed_urls: Mapped[int] = mapped_column(default=0)

    # Error handling
    error_message: Mapped[str | None] = mapped_column(nullable=True)

    # Shareable results
    shareable_id: Mapped[uuid.UUID] = mapped_column(UUID, unique=True, index=True, nullable=False, server_default=text("gen_random_uuid()"))
    is_public: Mapped[bool] = mapped_column(default=False)

    # Timestamps
    started_at: Mapped[datetime | None] = mapped_column(nullable=True)
    completed_at: Mapped[datetime | None] = mapped_column(nullable=True)

class ScrapedData(BaseModel):
    __tablename__ = "scraped_data"

    url: Mapped[str] = mapped_column(unique=True, index=True)
    s3_key: Mapped[str]  # S3 object key for raw scraped JSON

class GFWLTeamSubmission(BaseModel):
    __tablename__ = "gfwl_team_submissions"

    job_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("jobs.id"))
    team_name: Mapped[str]
    discovered_players: Mapped[list[str]] = mapped_column(JSON)
    confirmed_players: Mapped[list[str] | None] = mapped_column(JSON, nullable=True)
    confirmation_status: Mapped[str] = mapped_column(default="pending")  # pending, confirmed
```

## API Endpoints Design

### Authentication

- All endpoints require valid Clerk JWT token
- User context extracted from JWT claims

### Individual Mode Endpoints

```python
POST /api/v1/jobs/individual
# Submit URLs for individual player analysis
# Body: {"urls": ["url1", "url2", ...]}
# Returns: {"job_id": "uuid", "status": "pending"}

GET /api/v1/jobs/{job_id}
# Get job status
# Returns: Job details with current status (no transformed results)

GET /api/v1/jobs/{job_id}/results
# Get transformed results (on-demand processing)
# Returns: Both summary and detailed analysis results from raw S3 data

GET /api/v1/jobs/{job_id}/progress
# Get real-time progress updates
# Returns: {"processed": 5, "total": 10, "status": "running"}
```

### GFWL Mode Endpoints

```python
POST /api/v1/jobs/gfwl/submit-team
# Submit team data for analysis
# Body: {"team_name": "...", "weekly_data": {...}}
# Returns: {"job_id": "uuid", "status": "pending"}

GET /api/v1/jobs/gfwl/{job_id}/discovered-players
# Get list of discovered players (after initial processing)
# Returns: {"players": ["player1", "player2", ...], "status": "awaiting_confirmation"}

GET /api/v1/jobs/gfwl/{job_id}/results
# Get GFWL team analysis results (on-demand processing)
# Returns: Team analysis and individual player profiles for confirmed players from raw S3 data

POST /api/v1/jobs/gfwl/{job_id}/confirm-players
# Confirm which players to profile
# Body: {"confirmed_players": ["player1", "player3"]}
# Returns: {"status": "confirmed", "processing_started": true}
```

### General Job Management

```python
GET /api/v1/users/me/jobs
# Get all jobs for authenticated user
# Query params: ?status=completed&job_type=individual
# Returns: Paginated list of user's jobs

DELETE /api/v1/jobs/{job_id}
# Cancel a pending/running job
# Returns: {"status": "cancelled"}

POST /api/v1/jobs/{job_id}/share
# Enable/disable public sharing for completed job
# Body: {"is_public": true}
# Returns: {"shareable_id": "uuid", "is_public": true, "share_url": "/results/uuid"}

GET /api/v1/results/{shareable_id}
# Get shared job results (no authentication required)
# Returns: Transformed analysis results if job is public
```

## Celery Task Architecture

### Core Tasks

```python
@celery_app.task(bind=True, max_retries=3, default_retry_delay=60, retry_backoff=True)
def scrape_single_url(self, job_id: str, url: str) -> str:
    """Scrape a single URL and save to S3 (1+ minute per URL due to captcha bypass)"""
    try:
        # 1. Check if URL already exists in S3 (ScrapedData table)
        # 2. If exists, update job progress and return S3 key
        # 3. If not, scrape via BrightData (using ReplayExtractor logic)
        # 4. Save raw JSON to S3
        # 5. Create ScrapedData record with URL -> S3 key mapping
        # 6. Update job progress (increment processed_urls)
        # 7. Check if job is complete and update status
        # 8. Return S3 key
    except Exception as exc:
        # Retry with exponential backoff on failure
        raise self.retry(exc=exc)

@celery_app.task
def process_individual_job(job_id: str):
    """Orchestrate individual mode job by queuing URL tasks"""
    # 1. Get job and validate URLs
    # 2. Set job status to RUNNING
    # 3. Queue individual scrape_single_url tasks for each URL
    # 4. Each URL task will update progress independently
    # Note: Job completes when all URL tasks finish

@celery_app.task
def process_gfwl_discovery(job_id: str):
    """First phase of GFWL: discover players from team data"""
    # 1. Extract URLs from team data
    # 2. Queue scrape_single_url tasks for each URL
    # 3. Wait for all scraping to complete
    # 4. Parse raw data to extract unique player names
    # 5. Update job with discovered players
    # 6. Set status to awaiting_confirmation

@celery_app.task
def process_gfwl_profiles(job_id: str):
    """Second phase of GFWL: finalize after player confirmation"""
    # 1. Ensure all raw data is scraped and in S3
    # 2. Update job status to completed
    # Note: Player profiles generated on-demand when user requests results
```

### Task Orchestration

```python
# Individual job workflow - one task per URL for better progress tracking
def start_individual_job(job_id: str, urls: list[str]):
    # Queue individual URL scraping tasks
    for url in urls:
        scrape_single_url.delay(job_id, url)

# GFWL job workflow
def start_gfwl_job(job_id: str):
    # Phase 1: Discovery
    process_gfwl_discovery.delay(job_id)
    # Phase 2: Triggered by user confirmation endpoint

def confirm_gfwl_players(job_id: str):
    # Called when user confirms players
    process_gfwl_profiles.delay(job_id)
```

## On-Demand Transformation Service

### Core Transformation Functions

```python
class TransformationService:
    """Service for transforming raw S3 data into analysis results"""

        async def transform_individual_results(
        self,
        job: Job
    ) -> dict:
        """Transform individual job results from raw S3 data"""
        # 1. Get all S3 keys for job URLs from ScrapedData table
        # 2. Download raw JSON data from S3
        # 3. Parse and aggregate data for the target player
        # 4. Generate game-by-game analysis
        # 5. Create series summaries
        # 6. Return both summary and detailed results together

        async def transform_gfwl_results(
        self,
        job: Job
    ) -> dict:
        """Transform GFWL job results for confirmed players"""
        # 1. Use job.confirmed_players from GFWLTeamSubmission
        # 2. Get all S3 keys for job URLs from ScrapedData table
        # 3. Download and parse raw JSON data
        # 4. Filter data for confirmed players only
        # 5. Generate individual player profiles
        # 6. Create team analysis summary
        # 7. Return comprehensive team report

    async def discover_players_from_raw_data(self, job: Job) -> list[str]:
        """Extract unique player names from raw scraped data"""
        # 1. Get all S3 keys for job URLs
        # 2. Download raw JSON data
        # 3. Parse to extract all unique player names
        # 4. Return deduplicated player list

    async def get_raw_data_for_urls(self, urls: list[str]) -> dict[str, dict]:
        """Fetch raw data from S3 for given URLs"""
        # 1. Query ScrapedData table for S3 keys
        # 2. Download JSON data from S3
        # 3. Return mapping of URL -> raw_data
```

### Caching Strategy for Transformations

```python
# Use Redis for caching transformed results with TTL
cache_key = f"job_results:{job_id}:{job_type}"
cached_result = await redis.get(cache_key)
if cached_result:
    return cached_result

# If not cached, transform and cache with 1-hour TTL
result = await transform_data(...)
await redis.setex(cache_key, 3600, result)
return result
```

### Shareable Results Architecture

```python
class ShareService:
    """Service for managing shareable job results"""

    async def enable_sharing(self, job: Job) -> dict:
        """Enable public sharing for a completed job"""
        # 1. Verify job is completed and belongs to user
        # 2. Set is_public = True
        # 3. Return shareable_id and share URL

    async def get_shared_result(self, shareable_id: str) -> dict:
        """Get publicly shared job results"""
        # 1. Find job by shareable_id
        # 2. Verify is_public = True
        # 3. Transform and return results (with caching)
        # 4. No authentication required
```

## Configuration Requirements

### Environment Variables

```python
class Settings(BaseSettings):
    # Existing config...

    # Clerk Authentication
    CLERK_JWT_ISSUER: str
    CLERK_JWT_AUDIENCE: str
    CLERK_SECRET_KEY: str

    # AWS S3
    AWS_ACCESS_KEY_ID: str
    AWS_SECRET_ACCESS_KEY: str
    AWS_S3_BUCKET: str
    AWS_S3_REGION: str = "us-east-1"

    # Redis (Celery broker/backend + caching)
    REDIS_URL: str

    @property
    def CELERY_BROKER_URL(self) -> str:
        return self.REDIS_URL

    @property
    def CELERY_RESULT_BACKEND(self) -> str:
        return self.REDIS_URL

    # BrightData
    BRIGHTDATA_USERNAME: str
    BRIGHTDATA_PASSWORD: str
    BRIGHTDATA_ENDPOINT: str

    # Job Configuration
    MAX_URLS_PER_JOB: int = 100
    JOB_TIMEOUT_MINUTES: int = 60
    RESULTS_RETENTION_DAYS: int = 30
```

## Security & Performance Considerations

### Rate Limiting & Performance

- Max 5 job submissions per user per hour
- Max 12 URLs per individual job
- Max 100 URLs per GFWL job
- Implement request throttling for expensive operations
- **Long-running URL scrapes**: Each URL takes 1+ minutes due to BrightData captcha bypass
- **One task per URL**: Enables granular progress tracking and better fault tolerance
- **Task timeouts**: Set Celery task timeout to 5+ minutes per URL
- **Concurrency limits**: Limit concurrent BrightData requests to avoid rate limiting
- **Progress Updates**: Polling-based progress tracking (no WebSockets)

### Error Handling

- **Celery Retries**: Use built-in Celery retry mechanism with exponential backoff
- **Task Configuration**: `max_retries=3, default_retry_delay=60, retry_backoff=True`
- Graceful degradation when BrightData is unavailable
- Comprehensive error logging and user notifications
- Failed URL tasks don't block other URLs in the same job

### Caching Strategy

- **S3 Raw Data Cache**: Store scraped raw JSON data in S3, check ScrapedData table before scraping
- **Redis Transformation Cache**: Cache transformed results with TTL (1 hour) to avoid reprocessing
- **Database Query Optimization**: Optimize job listings and status queries
- **Lazy Loading**: Only transform data when specifically requested by users

### Security

- JWT token validation on all endpoints
- Input validation and sanitization
- Secure S3 bucket policies
- Rate limiting and DDoS protection

## Data Flow Architecture

### Individual Mode Flow

1. User submits job with URLs
2. Job created with PENDING status
3. Individual scrape_single_url tasks queued (one per URL)
4. Each task (1+ min): checks S3 cache → scrapes if needed → saves raw JSON → updates progress
5. Job marked COMPLETED when all URLs processed
6. User requests results → Redis cache check → transform raw S3 data if needed → return results

### GFWL Mode Flow

1. User submits team data
2. Discovery phase: scrape all team URLs individually
3. Parse raw data to discover players → update job with player list
4. User confirms players via separate endpoint
5. Job marked COMPLETED
6. User requests results → transform raw data for confirmed players only

## Railway Deployment Requirements

### Dockerfile Configuration

```dockerfile
FROM python:3.12-slim

# Install system dependencies for Playwright
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Install Playwright browsers
RUN playwright install chromium
RUN playwright install-deps chromium

# Set up app
WORKDIR /app
COPY . .

EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Railway Services

1. **Backend Service**: FastAPI app with Playwright
2. **PostgreSQL Database**: Railway PostgreSQL addon
3. **Redis Service**: Railway Redis addon
4. **Celery Worker**: Separate service running same image with different command

### Environment Variables for Railway

```bash
# Database (auto-provided by Railway)
DATABASE_URL=postgresql://...

# Redis (auto-provided by Railway)
REDIS_URL=redis://...

# Clerk
CLERK_JWT_ISSUER=...
CLERK_JWT_AUDIENCE=...
CLERK_SECRET_KEY=...

# AWS S3
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_S3_BUCKET=...

# BrightData
BRIGHTDATA_USERNAME=...
BRIGHTDATA_PASSWORD=...
BRIGHTDATA_ENDPOINT=...
```

### Railway Configuration

```toml
# railway.toml
[build]
builder = "dockerfile"

[deploy]
startCommand = "uvicorn app.main:app --host 0.0.0.0 --port $PORT"

# Celery worker service
[[services]]
name = "celery-worker"
startCommand = "celery -A app.celery_app worker --loglevel=info"
```

## Implementation Priority

### Phase 1: Core Infrastructure

1. ✅ Basic FastAPI setup
2. Database models and migrations (with shareable results)
3. Clerk JWT authentication
4. Basic job CRUD endpoints
5. Public shareable results endpoint

### Phase 2: Individual Mode

1. Individual job submission endpoint
2. Basic Celery task setup with retries
3. S3 integration
4. Dummy scraping logic
5. Job status and progress tracking

### Phase 3: GFWL Mode

1. GFWL workflow endpoints
2. Player discovery logic
3. Confirmation workflow
4. Profile generation

### Phase 4: Production Features

1. **ReplayExtractor Integration**: Refactor existing @replay_extractor.py ReplayExtractor class into Celery tasks
2. **Railway Deployment**: Dockerfile with Playwright, multi-service setup
3. Performance optimization and monitoring
4. Rate limiting and concurrency controls

## Confirmed Requirements

1. **GFWL Team Data Structure**: ✅ Will be provided later (Individual mode first)

2. **Real-time Updates**: ✅ Polling-based progress tracking (no WebSockets)

3. **Result Format**: ✅ JSON format for both modes (structure to be provided later)

4. **BrightData Integration**: ✅ Core scraping logic will be provided (ReplayExtractor)

5. **User Management**: ✅ Job history limits (max 50 jobs per user, auto-cleanup)

6. **File Downloads**: ✅ No download endpoints needed

## Next Steps - Development Roadmap

This planning document provides a comprehensive roadmap for your Duel Insights API. All requirements are confirmed and ready for implementation.

# Duel Insights API - Comprehensive Planning Document

## Overview

This FastAPI application serves as the backend for a NextJS frontend, focusing on submitting, processing, saving, and outputting jobs via Celery workers and AWS S3. The main functionality revolves around scraping and analyzing DuelingBook replay data.

## Tech Stack

- **Framework**: FastAPI
- **Authentication**: JWT auth via Clerk's JWT template
- **Task Queue**: Celery workers
- **Message Broker & Cache**: Redis (Celery broker/backend + result caching)
- **Storage**: AWS S3
- **Database**: PostgreSQL
- **Web Automation**: Playwright (for BrightData scraping)
- **Testing**: pytest
- **Infrastructure**: Railway (Backend + PostgreSQL + Redis)

## Core Features

### Job Types

#### 1. Individual Mode

- Users submit a list of URLs to analyze one player
- All replays must be from the same player
- Scrapes, parses, and transforms data game-by-game
- Outputs series summaries

#### 2. GFWL (Goat Format War League) Mode

- Users submit team results JSON with week-to-week URLs
- System finds all players in the matches
- User confirms which players to profile
- Creates comprehensive player profiles

## Data Models

### Design Philosophy

**Raw Data Storage + On-Demand Transformation Approach:**

- **Raw JSON Storage**: All scraped data stored as raw JSON in S3, referenced by ScrapedData table
- **No Result Duplication**: Job results not stored in database to avoid data duplication across overlapping URLs
- **On-Demand Processing**: Results computed in real-time when requested, allowing for flexible aggregation
- **Efficient Caching**: Transformed results cached in Redis with TTL to balance performance and freshness
- **Flexible Analysis**: Users can request different player combinations for GFWL jobs without re-scraping

### Core Models

```python
class JobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class JobType(str, Enum):
    INDIVIDUAL = "individual"
    GFWL = "gfwl"

class Job(BaseModel):
    __tablename__ = "jobs"

    job_type: Mapped[JobType]
    status: Mapped[JobStatus] = mapped_column(default=JobStatus.PENDING)
    user_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("users.id"))

    # Input data
    urls: Mapped[list[str]] = mapped_column(JSON)
    team_data: Mapped[dict | None] = mapped_column(JSON, nullable=True)  # For GFWL mode

    # Progress tracking
    total_urls: Mapped[int]
    processed_urls: Mapped[int] = mapped_column(default=0)

    # Error handling
    error_message: Mapped[str | None] = mapped_column(nullable=True)

    # Shareable results
    shareable_id: Mapped[uuid.UUID] = mapped_column(UUID, unique=True, index=True, nullable=False, server_default=text("gen_random_uuid()"))
    is_public: Mapped[bool] = mapped_column(default=False)

    # Timestamps
    started_at: Mapped[datetime | None] = mapped_column(nullable=True)
    completed_at: Mapped[datetime | None] = mapped_column(nullable=True)

class ScrapedData(BaseModel):
    __tablename__ = "scraped_data"

    url: Mapped[str] = mapped_column(unique=True, index=True)
    s3_key: Mapped[str]  # S3 object key for raw scraped JSON

class GFWLTeamSubmission(BaseModel):
    __tablename__ = "gfwl_team_submissions"

    job_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("jobs.id"))
    team_name: Mapped[str]
    discovered_players: Mapped[list[str]] = mapped_column(JSON)
    confirmed_players: Mapped[list[str] | None] = mapped_column(JSON, nullable=True)
    confirmation_status: Mapped[str] = mapped_column(default="pending")  # pending, confirmed
```

## API Endpoints Design

### Authentication

- All endpoints require valid Clerk JWT token
- User context extracted from JWT claims

### Individual Mode Endpoints

```python
POST /api/v1/jobs/individual
# Submit URLs for individual player analysis
# Body: {"urls": ["url1", "url2", ...]}
# Returns: {"job_id": "uuid", "status": "pending"}

GET /api/v1/jobs/{job_id}
# Get job status
# Returns: Job details with current status (no transformed results)

GET /api/v1/jobs/{job_id}/results
# Get transformed results (on-demand processing)
# Returns: Both summary and detailed analysis results from raw S3 data

GET /api/v1/jobs/{job_id}/progress
# Get real-time progress updates
# Returns: {"processed": 5, "total": 10, "status": "running"}
```

### GFWL Mode Endpoints

```python
POST /api/v1/jobs/gfwl/submit-team
# Submit team data for analysis
# Body: {"team_name": "...", "weekly_data": {...}}
# Returns: {"job_id": "uuid", "status": "pending"}

GET /api/v1/jobs/gfwl/{job_id}/discovered-players
# Get list of discovered players (after initial processing)
# Returns: {"players": ["player1", "player2", ...], "status": "awaiting_confirmation"}

GET /api/v1/jobs/gfwl/{job_id}/results
# Get GFWL team analysis results (on-demand processing)
# Returns: Team analysis and individual player profiles for confirmed players from raw S3 data

POST /api/v1/jobs/gfwl/{job_id}/confirm-players
# Confirm which players to profile
# Body: {"confirmed_players": ["player1", "player3"]}
# Returns: {"status": "confirmed", "processing_started": true}
```

### General Job Management

```python
GET /api/v1/users/me/jobs
# Get all jobs for authenticated user
# Query params: ?status=completed&job_type=individual
# Returns: Paginated list of user's jobs

DELETE /api/v1/jobs/{job_id}
# Cancel a pending/running job
# Returns: {"status": "cancelled"}

POST /api/v1/jobs/{job_id}/share
# Enable/disable public sharing for completed job
# Body: {"is_public": true}
# Returns: {"shareable_id": "uuid", "is_public": true, "share_url": "/results/uuid"}

GET /api/v1/results/{shareable_id}
# Get shared job results (no authentication required)
# Returns: Transformed analysis results if job is public
```

## Celery Task Architecture

### Core Tasks

```python
@celery_app.task(bind=True, max_retries=3, default_retry_delay=60, retry_backoff=True)
def scrape_single_url(self, job_id: str, url: str) -> str:
    """Scrape a single URL and save to S3 (1+ minute per URL due to captcha bypass)"""
    try:
        # 1. Check if URL already exists in S3 (ScrapedData table)
        # 2. If exists, update job progress and return S3 key
        # 3. If not, scrape via BrightData (using ReplayExtractor logic)
        # 4. Save raw JSON to S3
        # 5. Create ScrapedData record with URL -> S3 key mapping
        # 6. Update job progress (increment processed_urls)
        # 7. Check if job is complete and update status
        # 8. Return S3 key
    except Exception as exc:
        # Retry with exponential backoff on failure
        raise self.retry(exc=exc)

@celery_app.task
def process_individual_job(job_id: str):
    """Orchestrate individual mode job by queuing URL tasks"""
    # 1. Get job and validate URLs
    # 2. Set job status to RUNNING
    # 3. Queue individual scrape_single_url tasks for each URL
    # 4. Each URL task will update progress independently
    # Note: Job completes when all URL tasks finish

@celery_app.task
def process_gfwl_discovery(job_id: str):
    """First phase of GFWL: discover players from team data"""
    # 1. Extract URLs from team data
    # 2. Queue scrape_single_url tasks for each URL
    # 3. Wait for all scraping to complete
    # 4. Parse raw data to extract unique player names
    # 5. Update job with discovered players
    # 6. Set status to awaiting_confirmation

@celery_app.task
def process_gfwl_profiles(job_id: str):
    """Second phase of GFWL: finalize after player confirmation"""
    # 1. Ensure all raw data is scraped and in S3
    # 2. Update job status to completed
    # Note: Player profiles generated on-demand when user requests results
```

### Task Orchestration

```python
# Individual job workflow - one task per URL for better progress tracking
def start_individual_job(job_id: str, urls: list[str]):
    # Queue individual URL scraping tasks
    for url in urls:
        scrape_single_url.delay(job_id, url)

# GFWL job workflow
def start_gfwl_job(job_id: str):
    # Phase 1: Discovery
    process_gfwl_discovery.delay(job_id)
    # Phase 2: Triggered by user confirmation endpoint

def confirm_gfwl_players(job_id: str):
    # Called when user confirms players
    process_gfwl_profiles.delay(job_id)
```

## On-Demand Transformation Service

### Core Transformation Functions

```python
class TransformationService:
    """Service for transforming raw S3 data into analysis results"""

        async def transform_individual_results(
        self,
        job: Job
    ) -> dict:
        """Transform individual job results from raw S3 data"""
        # 1. Get all S3 keys for job URLs from ScrapedData table
        # 2. Download raw JSON data from S3
        # 3. Parse and aggregate data for the target player
        # 4. Generate game-by-game analysis
        # 5. Create series summaries
        # 6. Return both summary and detailed results together

        async def transform_gfwl_results(
        self,
        job: Job
    ) -> dict:
        """Transform GFWL job results for confirmed players"""
        # 1. Use job.confirmed_players from GFWLTeamSubmission
        # 2. Get all S3 keys for job URLs from ScrapedData table
        # 3. Download and parse raw JSON data
        # 4. Filter data for confirmed players only
        # 5. Generate individual player profiles
        # 6. Create team analysis summary
        # 7. Return comprehensive team report

    async def discover_players_from_raw_data(self, job: Job) -> list[str]:
        """Extract unique player names from raw scraped data"""
        # 1. Get all S3 keys for job URLs
        # 2. Download raw JSON data
        # 3. Parse to extract all unique player names
        # 4. Return deduplicated player list

    async def get_raw_data_for_urls(self, urls: list[str]) -> dict[str, dict]:
        """Fetch raw data from S3 for given URLs"""
        # 1. Query ScrapedData table for S3 keys
        # 2. Download JSON data from S3
        # 3. Return mapping of URL -> raw_data
```

### Caching Strategy for Transformations

```python
# Use Redis for caching transformed results with TTL
cache_key = f"job_results:{job_id}:{job_type}"
cached_result = await redis.get(cache_key)
if cached_result:
    return cached_result

# If not cached, transform and cache with 1-hour TTL
result = await transform_data(...)
await redis.setex(cache_key, 3600, result)
return result
```

### Shareable Results Architecture

```python
class ShareService:
    """Service for managing shareable job results"""

    async def enable_sharing(self, job: Job) -> dict:
        """Enable public sharing for a completed job"""
        # 1. Verify job is completed and belongs to user
        # 2. Set is_public = True
        # 3. Return shareable_id and share URL

    async def get_shared_result(self, shareable_id: str) -> dict:
        """Get publicly shared job results"""
        # 1. Find job by shareable_id
        # 2. Verify is_public = True
        # 3. Transform and return results (with caching)
        # 4. No authentication required
```

## Configuration Requirements

### Environment Variables

```python
class Settings(BaseSettings):
    # Existing config...

    # Clerk Authentication
    CLERK_JWT_ISSUER: str
    CLERK_JWT_AUDIENCE: str
    CLERK_SECRET_KEY: str

    # AWS S3
    AWS_ACCESS_KEY_ID: str
    AWS_SECRET_ACCESS_KEY: str
    AWS_S3_BUCKET: str
    AWS_S3_REGION: str = "us-east-1"

    # Redis (Celery broker/backend + caching)
    REDIS_URL: str

    @property
    def CELERY_BROKER_URL(self) -> str:
        return self.REDIS_URL

    @property
    def CELERY_RESULT_BACKEND(self) -> str:
        return self.REDIS_URL

    # BrightData
    BRIGHTDATA_USERNAME: str
    BRIGHTDATA_PASSWORD: str
    BRIGHTDATA_ENDPOINT: str

    # Job Configuration
    MAX_URLS_PER_JOB: int = 100
    JOB_TIMEOUT_MINUTES: int = 60
    RESULTS_RETENTION_DAYS: int = 30
```

## Security & Performance Considerations

### Rate Limiting & Performance

- Max 5 job submissions per user per hour
- Max 12 URLs per individual job
- Max 100 URLs per GFWL job
- Implement request throttling for expensive operations
- **Long-running URL scrapes**: Each URL takes 1+ minutes due to BrightData captcha bypass
- **One task per URL**: Enables granular progress tracking and better fault tolerance
- **Task timeouts**: Set Celery task timeout to 5+ minutes per URL
- **Concurrency limits**: Limit concurrent BrightData requests to avoid rate limiting
- **Progress Updates**: Polling-based progress tracking (no WebSockets)

### Error Handling

- **Celery Retries**: Use built-in Celery retry mechanism with exponential backoff
- **Task Configuration**: `max_retries=3, default_retry_delay=60, retry_backoff=True`
- Graceful degradation when BrightData is unavailable
- Comprehensive error logging and user notifications
- Failed URL tasks don't block other URLs in the same job

### Caching Strategy

- **S3 Raw Data Cache**: Store scraped raw JSON data in S3, check ScrapedData table before scraping
- **Redis Transformation Cache**: Cache transformed results with TTL (1 hour) to avoid reprocessing
- **Database Query Optimization**: Optimize job listings and status queries
- **Lazy Loading**: Only transform data when specifically requested by users

### Security

- JWT token validation on all endpoints
- Input validation and sanitization
- Secure S3 bucket policies
- Rate limiting and DDoS protection

## Data Flow Architecture

### Individual Mode Flow

1. User submits job with URLs
2. Job created with PENDING status
3. Individual scrape_single_url tasks queued (one per URL)
4. Each task (1+ min): checks S3 cache → scrapes if needed → saves raw JSON → updates progress
5. Job marked COMPLETED when all URLs processed
6. User requests results → Redis cache check → transform raw S3 data if needed → return results

### GFWL Mode Flow

1. User submits team data
2. Discovery phase: scrape all team URLs individually
3. Parse raw data to discover players → update job with player list
4. User confirms players via separate endpoint
5. Job marked COMPLETED
6. User requests results → transform raw data for confirmed players only

## Railway Deployment Requirements

### Dockerfile Configuration

```dockerfile
FROM python:3.12-slim

# Install system dependencies for Playwright
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Install Playwright browsers
RUN playwright install chromium
RUN playwright install-deps chromium

# Set up app
WORKDIR /app
COPY . .

EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Railway Services

1. **Backend Service**: FastAPI app with Playwright
2. **PostgreSQL Database**: Railway PostgreSQL addon
3. **Redis Service**: Railway Redis addon
4. **Celery Worker**: Separate service running same image with different command

### Environment Variables for Railway

```bash
# Database (auto-provided by Railway)
DATABASE_URL=postgresql://...

# Redis (auto-provided by Railway)
REDIS_URL=redis://...

# Clerk
CLERK_JWT_ISSUER=...
CLERK_JWT_AUDIENCE=...
CLERK_SECRET_KEY=...

# AWS S3
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_S3_BUCKET=...

# BrightData
BRIGHTDATA_USERNAME=...
BRIGHTDATA_PASSWORD=...
BRIGHTDATA_ENDPOINT=...
```

### Railway Configuration

```toml
# railway.toml
[build]
builder = "dockerfile"

[deploy]
startCommand = "uvicorn app.main:app --host 0.0.0.0 --port $PORT"

# Celery worker service
[[services]]
name = "celery-worker"
startCommand = "celery -A app.celery_app worker --loglevel=info"
```

## Implementation Priority

### Phase 1: Core Infrastructure

1. ✅ Basic FastAPI setup
2. Database models and migrations (with shareable results)
3. Clerk JWT authentication
4. Basic job CRUD endpoints
5. Public shareable results endpoint

### Phase 2: Individual Mode

1. Individual job submission endpoint
2. Basic Celery task setup with retries
3. S3 integration
4. Dummy scraping logic
5. Job status and progress tracking

### Phase 3: GFWL Mode

1. GFWL workflow endpoints
2. Player discovery logic
3. Confirmation workflow
4. Profile generation

### Phase 4: Production Features

1. **ReplayExtractor Integration**: Refactor existing @replay_extractor.py ReplayExtractor class into Celery tasks
2. **Railway Deployment**: Dockerfile with Playwright, multi-service setup
3. Performance optimization and monitoring
4. Rate limiting and concurrency controls

## Confirmed Requirements

1. **GFWL Team Data Structure**: ✅ Will be provided later (Individual mode first)

2. **Real-time Updates**: ✅ Polling-based progress tracking (no WebSockets)

3. **Result Format**: ✅ JSON format for both modes (structure to be provided later)

4. **BrightData Integration**: ✅ Core scraping logic will be provided (ReplayExtractor)

5. **User Management**: ✅ Job history limits (max 50 jobs per user, auto-cleanup)

6. **File Downloads**: ✅ No download endpoints needed

## Next Steps - Development Roadmap

This planning document provides a comprehensive roadmap for your Duel Insights API. All requirements are confirmed and ready for implementation.
